---
title: "November 18"
output: 
  bookdown::pdf_document2:
    number_sections: false
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE, dev = "ragg_png", dpi=200)
library(tidyverse)
library(patchwork)

source(here::here("R", "updates.R"))
# Sys.setenv(TAR_PROJECT = "ale")
# tar_config_set(store = here::here("_ale"))


png_to_patch <- function(img, pix=NULL){
  png::readPNG(img) |>
    as.raster() |>
    patchwork::wrap_elements()
}
```


## A2CPS


### UC SNR

With UC, there are two features to explain: why is the anatomical SNR higher, and why is the SNR more variable. I think we've found an answer to both of these that relates to features of the data that Vince mentioned at the DIRC meeting, about a possible dynamic range issue. For anatomical scans from UC, most of the background has intensity of 0 (Figure \@ref(fig:anatUC)), but most of the background is non-zero for the other sites.

```{r, anatUC, echo=FALSE, out.width='100%', fig.cap="Anatomical for UC10036V1. Pixels are colored such that the lowest non-zero intensities are pink and the zero intensities are black. Most voxels outside of the brian have zero intensity."}
knitr::include_graphics('img/UC10036V1_T1w.png')
```

That most air voxels are being forced to 0 in UC scans could cause the SNR for UC to appear to be more variable. MRIQC calculates the "noise" in with the variability of the signal in an air mask. Voxels in that air mask with intensity of 0 are excluded. So, when more of the background is 0, there are fewer voxels used to calculate the noise (Figure \@ref(fig:anatUC2)). The remaining voxels tend to be near the head (e.g., pink regions in Figure \@ref(fig:anatUC)). Voxels near the head have a relatively high intensity and standard deviation, whereas voxels far from the head have low intensity and standard deviation. So for UC, when the air mask is relatively small, the noise in SNR is calculated with only a few noisy voxels, driving down the estimated SNR. But when the air mask is relatively large, the noise in SNR is calculated with this same set of noisy voxels in addition to a larger set that is relatively quiet, driving up the estimated SNR. But whether a voxel in the background will be represented with 0 or some other value seems to be variable, causing spurious variability in the air mask and SNR distribution.

```{r, anatUC2, echo=FALSE, out.width='100%', fig.cap="Examples of Smaller and Larger Hat Masks. Regions inside the red shape are used to calculate noise in SNR calculation. These shapes exclude voxels with intensity 0. "}
(png_to_patch('img/sub-10103_ses-V1_T1w-air.png') + ggtitle("UC10103V1")) / 
(png_to_patch('img/sub-10102_ses-V1_T1w-air.png')+ ggtitle("UC10102V1") )
```

I'm not sure whether the abundance of intensity 0 voxels in the UC data is a feature or issue. The underlying data are stored as 16-bit integers, which limits the range of values that can be used to represent voxel intensities. One way in which these 0 values could be a feature is that it enables better precision of voxels in the brain. It may also be due to differences in how the data are converted into DICOMs. Tanmay and Jamie have checked Philips T1 scans from other datasets. Interestingly, many background voxels from those scans also have intensity 0. So, this might not be a UC-specific thing, but instead a (possibly known) aspect of working with Philips data.

A summary is that the variability in SNR may be due to the range of intensities used to represent the data in Philips scans, and specifically thresholding at the low end. In that case, it's unclear whether the UC scans actually have higher SNR (they still may), or whether the reported values are spurious (it's harder to estimate SNR with so many voxels missing intensity values). 

### "Low" SNR at NS, UI, and UM

There was some concern expressed that the absolute value of the SNR at the other sites may be low. I don't have a solid explanation for the SNR magnitude at NS, UI, and UM. I think the most valuable information would be the MRIQC metrics for the ABCD dataset, since SNR will depend on the scanning protocol and the A2CPS protocol is close to the one used in ABCD. Another useful comparison is against the datasets, like the datasets Tor shared. That latter comparison supports the claim that SNR for the non-UC sites appears low. Note that 1) we haven't seen scans with lower SNR than have been shown at the imaging sites meetings, and 2) the traveling human scans have SNR values that are contained within the relevant site-wise distributions. So, the absolute value of SNR may not be critical, since the distributions within these sites still allow us to flag outliers -- to flag scans with problematic SNR.

### Ghosting at NS

The final point raised at the meeting was about ghosting. It sounds like there is a standard threshold for ghosting, that the ghost-to-signal ratio on a scanner must be lower than 5%. It sounds like that is estimated with a phantom, presumably using a standardized sequence. For the A2CPS data, the GSR as calculated by MRIQC for most sites is under %5, but for NS it is between 10-15%. It's worth noting these distributions are comparable to the GSR for the second traveling human; in each of the four functional scans at NS it's around 14%, and at UI it's around 2%. This suggests that the calculation is consistent between subjects and traveling human, but it leaves open whether MRIQC is picking up anything relevant in the data. 

To see whether MRIQC's GSR estimation tracks any meaningful feature of the data, I think it's worth looking at the comparison we showed in September's Imaging Sites meeting (reproduced in Figure \@ref(fig:gsr)). By highlighting the background, participants with high GSR (i.e., participants from NS) show evidence of artifact in the ghost-prone regions. Note that this artifact would be easily missed in a visual check of the raw data (e.g., top row of Figure \@ref(fig:gsr)). Back in September, a follow-up of looking at Phantom data was mentioned, followed by the possibility of consulting with a Siemens engineer. 

This artifact is evidence that the difference in GSR distributions is real, although the importance of that difference is still an open question.

```{r, gsr, fig.cap="Examples of Better and Worse Ghost-to-Signal Ratio. The first row shows the average signal (across time) in the raw image, for participant with a better (UI10034V1) and worse (NS10047V1) GSR. The bottom row shos the same averages, but the color scale has been shifted so that most voxels in the brain are saturated and the background variation is highlighted."}
better0 <- imager::load.image(here::here("analysis", "updates", "img", "sub-10034_z6.png")) |>
  # imager::imrotate(180) |>
  imager::imresize(4, interpolation = 6) |>
  imager::grayscale() |>
  # as.raster(colorscale = cscale) |>
  as.raster() |>
  patchwork::wrap_elements()

worse0 <- imager::load.image(here::here("analysis", "updates", "img", "sub-10047_z4.png")) |>
  # imager::imrotate(180) |>
  imager::imresize(4, interpolation = 6) |>
  imager::grayscale() |>
  as.raster() |>
  patchwork::wrap_elements()

better <- imager::load.image(here::here("analysis", "updates", "img", "better.png")) |>
  imager::imrotate(180, interpolation = 6) |>
  imager::imresize(4, interpolation = 6) |>
  imager::grayscale() |>
  # as.raster(colorscale = cscale) |>
  as.raster() |>
  patchwork::wrap_elements()

worse <- imager::load.image(here::here("analysis", "updates", "img", "worse.png")) |>
  imager::imrotate(180, interpolation = 6) |>
  imager::imresize(4, interpolation = 6) |>
  imager::grayscale() |>
  as.raster() |>
  patchwork::wrap_elements()


((better0 + ggtitle(label="better (UI10034V1)")) + (worse0 + ggtitle(label="worse (NS10047V1)"))) /
  (better + worse)

```


